---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```
# brolgar

**br**owse **o**ver **l**ongitudinal **d**ata **g**raphically and **a**nalytically in **R**

<!-- badges: start -->
[![Travis build status](https://travis-ci.org/njtierney/brolgar.svg?branch=master)](https://travis-ci.org/njtierney/brolgar)
[![AppVeyor build status](https://ci.appveyor.com/api/projects/status/github/njtierney/brolgar?branch=master&svg=true)](https://ci.appveyor.com/project/njtierney/brolgar)
[![Codecov test coverage](https://codecov.io/gh/njtierney/brolgar/branch/master/graph/badge.svg)](https://codecov.io/gh/njtierney/brolgar?branch=master)
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)
<!-- badges: end -->

Exploring longitudinal data can be challenging. For example, when there are many individuals it is difficult to look at all of them, as you often get a "plate of spaghetti" plot, with many lines plotted on top of each other. 

```{r show-spaghetti}
library(brolgar)
library(ggplot2)
ggplot(wages_ts, 
       aes(x = xp, 
             y = ln_wages, 
             group = id)) + 
  geom_line()
```

These are hard to interpret. 

What you want is to identify those interesting individual lines, so you can get something like the following:

```{r show-monotonic, echo = FALSE}
library(gghighlight)
library(ggplot2)
library(dplyr)
wages_ts %>%
  features(ln_wages, feat_monotonic) %>%
  left_join(wages_ts, by = "id") %>%
  ggplot(aes(x = xp,
             y = ln_wages,
             group = id)) +
  geom_line() + 
  gghighlight(increase)
```
```

This is what `brolgar` helps you do, by providing tools to:

* Calculate features (summaries) for each individual series
* Efficiently explore your raw data
* Evaluate statistical models.

## Installation

Install from [GitHub](https://github.com/) with:

``` r
# install.packages("remotes")
remotes::install_github("njtierney/brolgar")
```

# Data in `brolgar`

To efficiently look at your longitudinal data, we assume it **is a time series**, with irregular time periods between measurements. This might seem strange, (that's OK!), but there are **two important things** to remember:

1. The **key** variable in your data is the **identifier** of your individuals or series.
2. The **index** variable is the **time** component of your data.

Together, the **index** and **key** uniquely identify an observation.

The term `key` is used a lot in brolgar, so it is an important idea to internalise:

> **The key is the identifier of your individuals or series**


If you want to learn more about what longitudinal data as a time series, you can [read more in the vignette, "Longitudinal Data Structures"](library/brolgar/html/longitudinal-data-structures.html)

## Quickly exploring longitudinal data

### `sample_n_keys()`

Just as in `dplyr` how you can use `sample_n()` to sample `n` observations, you can take a random sample of `n keys` using `sample_n_keys()`:

```{r plot-sample-n-keys}
set.seed(2019-7-15-1300)
wages_ts %>%
  sample_n_keys(size = 10) %>%
  ggplot(aes(x = xp,
             y = ln_wages,
             group = id)) + 
  geom_line()
```

You could also combine with this `filter_n_obs` to only show keys with many observations:

```{r plot-filter-sample-n-keys}
set.seed(2019-7-15-1259)
wages_ts %>%
  filter_n_obs(n_obs > 5) %>%
  sample_n_keys(size = 10) %>%
  ggplot(aes(x = xp,
             y = ln_wages,
             group = id)) + 
  geom_line()
```

There is also `sample_frac_keys()`, which samples a fraction of available keys.

### `stratify_keys()`

To look at as much of the raw data as possible, it can be helpful to stratify the data into groups for plotting. You can `stratify` the `keys` using the `stratify_keys()` function, which adds the column, `.strata`:

```{r use-strata}
wages_ts %>%
  sample_n_keys(100) %>% 
  stratify_keys(n_strata = 10)
```

This then allows the user to create facetted plots showing a lot more of the raw data

```{r plot-strata}
set.seed(2019-07-15-1258)
wages_ts %>%
  sample_n_keys(120) %>% 
  stratify_keys(n_strata = 12) %>%
  ggplot(aes(x = xp,
             y = ln_wages,
             group = id)) + 
  geom_line() + 
  facet_wrap(~.strata)
```


## Exploratory modelling

You can fit a linear model for each key using `key_slope()`. This returns the intercept and slope estimate for each key, given some linear model formula. We can get the number of observations, and slope information for each individual to identify those that are decreasing over time. 

```{r use-gghighlight}
key_slope(wages_ts,ln_wages ~ xp)
```

We can then join these summaries back to the data:

```{r show-wages-lg}
library(dplyr)
wages_slope <- key_slope(wages_ts,ln_wages ~ xp) %>%
  left_join(wages_ts, by = "id") 

wages_slope
```

And highlight those individuals with a negative slope using `gghighlight`:

```{r use-gg-highlight}
library(gghighlight)

wages_slope %>% 
  ggplot(aes(x = xp, 
             y = ln_wages, 
             group = id)) + 
  geom_line() +
  gghighlight(.slope_xp < 0)
```


### Find keys near other summaries with `keys_near()`

We could take our slope information and find those individuals who are representative of the min, median, maximum, etc of growth, using `keys_near()`:

```{r keys-near}
wages_slope %>%
  keys_near(key = id,
            var = .slope_xp,
            funs = l_three_num)
```

```{r keys-near-plot}
wages_slope %>%
  keys_near(key = id,
            var = .slope_xp,
            funs = l_three_num) %>%
  left_join(wages_ts, by = "id") %>%
  ggplot(aes(x = xp,
             y = ln_wages,
             group = id,
             colour = stat)) + 
  geom_line()
```


## Finding features in longitudinal data

You can extract `features` of longitudinal data using the `features` function, from `fablelite`. You can, for example, calculate the minimum of a given variable for each key by providing a named list like so:

```{r features-min}
wages_ts %>%
  features(ln_wages, 
           list(min = min))
```

You want to get the first and last values using `dplyr::first` and `dplyr::last`

```{r features-first-last}
library(dplyr)

wages_ts %>%
  features(ln_wages, 
           list(first = first,
                last = last))
```

`brolgar` provides some helper features. For example, creating the five number summary:

```{r features-five-num}
wages_ts %>%
  features(ln_wages, feat_five_num)
```

Or finding those whose values only increase or decrease with `monotonic`

```{r features-monotonic}
wages_ts %>%
  features(ln_wages, feat_monotonic)
```

## Linking individuals back to the data

You can join these features back to the data with `left_join`, like so:

```{r features-left-join}
wages_ts %>%
  features(ln_wages, feat_monotonic) %>%
  left_join(wages_ts, by = "id") %>%
  ggplot(aes(x = xp,
             y = ln_wages,
             group = id)) +
  geom_line() + 
  gghighlight(increase)
```

### `n_key_obs()`

We can calculate the number of observations for each `key`, using `n_key_obs()`:

```{r example-n-obs}
n_key_obs(wages_ts)
```

This returns a dataframe, with one row per key, and the number of observations for each key.

This could be further summarised to get a sense of the patterns of the number of observations:

```{r summarise-n-obs}
library(ggplot2)
n_key_obs(wages_ts) %>%
ggplot(aes(x = n_obs)) + 
  geom_bar()

n_key_obs(wages_ts) %>% summary()
```

### `add_n_key_obs()`

You can add information about the number of observations for each key with `add_n_key_obs()`:

```{r show-add-n-key-obs}
wages_ts %>% add_n_key_obs()
```

Which you can then use to filter observations:

```{r show-add-obs-filter}
wages_ts %>% 
  add_n_key_obs() %>%
  filter(n_obs > 3)
```

Alternatively, you can use the shortcut, `filter_n_obs()`:

```{r use-filter-n-obs}
wages_ts %>% 
  filter_n_obs(n_obs > 3)
```

# A Note on the API

This version of brolgar was been forked from [tprvan/brolgar](https://github.com/tprvan/brolgar), and has undergone breaking changes to the API.

<!-- These are referred to as a **longnostics**, a portmanteau of **long**itudinal and **cognostic**. These **longnostics** make it straightforward to extract subjects with certain properties to gain some insight into the data.  -->


<!-- But calculating this for individuals draws you away from your analysis, and instead you are now wrangling with a different problem: summarising key information about each individual and incorporating that back into the data.  -->

<!-- This is annoying, and distracts from your analysis, inviting errors. -->
