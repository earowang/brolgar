---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
library(brolgar)
data(wages_ts)
```
# brolgar

**br**owse **o**ver **l**ongitudinal **d**ata **g**raphically and **a**nalytically in **R**

<!-- badges: start -->
[![Travis build status](https://travis-ci.org/njtierney/brolgar.svg?branch=master)](https://travis-ci.org/njtierney/brolgar)
[![AppVeyor build status](https://ci.appveyor.com/api/projects/status/github/njtierney/brolgar?branch=master&svg=true)](https://ci.appveyor.com/project/njtierney/brolgar)
[![Codecov test coverage](https://codecov.io/gh/njtierney/brolgar/branch/master/graph/badge.svg)](https://codecov.io/gh/njtierney/brolgar?branch=master)
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)
<!-- badges: end -->

Exploring longitudinal data can be challenging. For example, when there are many individuals it is difficult to look at all of them, as you often get a "plate of spaghetti" plot, with many lines plotted on top of each other. 

```{r show-spaghetti}
library(brolgar)
library(ggplot2)
ggplot(wages_ts, 
       aes(x = xp, 
             y = ln_wages, 
             group = id)) + 
  geom_line()
```

These are hard to interpret. 

You might then want to explore those interesting individual lines. Say, those with high or low variation. But calculating this for individuals draws you away from your analysis, and instead you are now wrangling with a different problem: summarising key information about each individual and incorporating that back into the data. 

This is annoying, and distracts from your analysis, inviting errors.

**brolgar** provides tools to calculate statistical summaries for each individual, and methods to assist you in identifying individuals of interest. 

## Installation

Install from [GitHub](https://github.com/) with:

``` r
# install.packages("remotes")
remotes::install_github("njtierney/brolgar")
```

## Longitudinal data is time series data

A **Big Idea** in `brolgar` is that longitudinal data is a time series.

> Anything that is observed sequentially over time is a time series.
-- [Rob Hyndman and George Athanasopolous (Forecasting: Principles and Practice)](https://otexts.com/fpp2/data-methods.html)

Once we account for this time series structure inherent in longitudinal data, we gain access to a suite of nice tools that simplify and accelerate how we work with time series data.

## Converting your longitudinal data to a time series

To convert longitudinal data into a "**t**ime **s**eries tibble", a `tsibble`, we need to consider:

* What identifies the **time** component of the data? This is the **index**
* What is the **unique identifier** of an individual/series? This is the **key**

Together, the **index** and **key** uniquely identify an observation.

What do we mean by this? Let's look at the first section of the wages, **wages** data analysed in Singer & Willett (2003):

```{r slice-wages}
library(dplyr)
slice(wages_ts, 1:10)
```

We have the `id` column, which identifies an individual.

We also have the `xp` column, which identifies the experience an individual has.

So:

* key: `id`
* index: `exper`

We could create a `tsibble` of this data by using the `as_tsibble` function from `tsibble`, also stating, `regular = FALSE`, since we have an `irregular` time series:

```{r create-tsibble}
library(tsibble)
as_tsibble(x = wages_ts,
           key = id,
           index = xp,
           regular = FALSE)
```

Note the following information printed at the top of `wages_ts`

```
# A tsibble: 6,402 x 9 [!]
# Key:       id [888]
...
```

This says, we have `r nrow(wages_ts)` rows, with `r ncol(wages_ts)` columns. The `!` means that there is no regular spacing between series, and then our "key" is `id`, of which there `r n_keys(wages_ts)`.

## Quickly exploring longitudinal data

### `n_key_obs()`

We can calculate the number of observations for each `key`, using `n_key_obs()`:

```{r example-n-obs}
n_key_obs(wages_ts)
```

This returns one row per key, with the number of observations per key.

This could be further summarised to get a sense of the range of the data:

```{r summarise-n-obs}
library(ggplot2)
n_key_obs(wages_ts) %>%
ggplot(aes(x = n_obs)) + 
  geom_bar()

n_key_obs(wages_ts) %>% summary()
```

### `add_n_key_obs()`

You can add information about the number of observations for each key with `add_n_key_obs()`:

```{r show-add-n-key-obs}
wages_ts %>% add_n_key_obs()
```

Which you can then use to filter observations:

```{r show-add-obs-filter}
wages_ts %>% 
  add_n_key_obs() %>%
  filter(n_obs > 3)
```

Alternatively, you can use the shortcut, `filter_n_obs()`:

```{r use-filter-n-obs}
wages_ts %>% 
  filter_n_obs(n_obs > 3)
```

### `sample_n_keys()`

You can take a random sample of `n` keys using `sample_n_keys()`:

```{r plot-sample-n-keys}
wages_ts %>%
  sample_n_keys(size = 10) %>%
  ggplot(aes(x = xp,
             y = ln_wages,
             group = id)) + 
  geom_line()
```

You could also combine with this `filter_n_obs` to only show keys with many observations:

```{r plot-filter-sample-n-keys}
wages_ts %>%
  filter_n_obs(n_obs > 5) %>%
  sample_n_keys(size = 10) %>%
  ggplot(aes(x = xp,
             y = ln_wages,
             group = id)) + 
  geom_line()
```

There is also `sample_frac_keys()`, which allows for sampling a fraction of available keys.

### `stratify_keys()`

In keeping in the spirit of wanting to look at as much of the raw data as possible, it can be helpful to stratify the data into groups for plotting. You can `stratify` the `keys` using the `stratify_keys()` function, which adds the column, `.strata`:

```{r use-strata}
wages_ts %>%
  sample_n_keys(100) %>% 
  stratify_keys(n_strata = 10)
```

This then allows the user to create facetted plots showing a lot more of the raw data

```{r plot-strata}
wages_ts %>%
  sample_n_keys(120) %>% 
  stratify_keys(n_strata = 12) %>%
  ggplot(aes(x = xp,
             y = ln_wages,
             group = id)) + 
  geom_line() + 
  facet_wrap(~.strata)
```

## Exploratory modelling

You can fit a linear model for each key using `key_slope()`. This returns the intercept and slope estimate for each key, given some linear model formula. 

We can get the number of observations, and slope information for each individual to identify those that are decreasing over time. 

```{r use-gghighlight}
sl <- key_slope(wages_ts,ln_wages ~ xp)
ns <- n_key_obs(wages_ts)

sl
ns
```

We can then join these summaries back to the data:

```{r show-wages-lg}
wages_lg <- wages_ts %>%
  left_join(sl, by = "id") %>%
  left_join(ns, by = "id")

wages_lg
```

And highlight those individuals with more than 5 observations, and highlight those with a negative slope using `gghighlight`:

```{r use-gg-highlight}
library(gghighlight)

wages_lg %>% 
  filter(n_obs > 5) %>%
  ggplot(aes(x = xp, 
             y = ln_wages, 
             group = id)) + 
  geom_line() +
  gghighlight(.slope_xp < (-0.5),
              use_direct_label = FALSE)
```


### `l_summarise()`

## Finding features in longitudinal data

You can extract `features` of longitudinal data using the `features` function, which is imported from `fablelite` and `feasts`. You can, for example, calculate the minimum of a given variable for each key by providing a named list like so:

```{r features-min}
wages_ts %>%
  features(ln_wages, 
           list(min = min))
```

You want to get the first and last values using `dplyr::first` and `dplyr::last`

```{r features-first-last}
library(dplyr)

wages_ts %>%
  features(ln_wages, 
           list(first = first,
                last = last))
```

`brolgar` provides some helper features. For example, creating the five number summary:

```{r features-five-num}
wages_ts %>%
  features(ln_wages, feat_five_num)
```

Or finding those whos values only increase or decrease with `monotonic`

```{r features-monotonic}
wages_ts %>%
  features(ln_wages, feat_monotonic)
```

## Linking individuals back to the data

You can join these features back to the data with `left_join`, like so:

```{r features-left-join}
wages_ts %>%
  features(ln_wages, feat_monotonic) %>%
  left_join(wages_ts, by = "id") %>%
  filter(increase) %>%
  ggplot(aes(x = xp,
             y = ln_wages,
             group = id)) +
  geom_line()
```

# A Note on the API

This version of brolgar was been forked from [tprvan/brolgar](https://github.com/tprvan/brolgar), and has undergone breaking changes to the API.


<!-- These are referred to as a **longnostics**, a portmanteau of **long**itudinal and **cognostic**. These **longnostics** make it straightforward to extract subjects with certain properties to gain some insight into the data.  -->


