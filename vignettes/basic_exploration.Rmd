---
title: "Starting Steps"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Starting Steps}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r knitr-set-chunk, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(brolgar)
```

When we first get a longitudinal dataset, you need to understand some of its structure. This vignette demonstrates part of the process of understanding your new longitudinal data.

# Setting up your data

For `brolgar` to work best, you should convert your data into a time series `tsibble` using the `tsibble` package. You can do so by doing:

```{r}

```

To read more, see the vignette: [longitudinal-data-structures]()

# Basic summaries of the data

When you first get a dataset, you need to 

## How many observations are there?

The number of observations can mean different things
We can kind the number of keys using `n_keys()`:

```{r n-obs}
library(tsibble)
n_keys(wages_ts)
```

Note that this is a single number, in this case, we have `r n_keys(wages_ts)` observations.

However, we might want to know how many observations we have for each individual. If we want the number of observations in each key (id), then we can use `n_key_obs`. This is a special function from `brolgar`, indicated by `l_`. Every `l_` function will return us a dataframe.

```{r l-n-obs}
n_key_obs(wages_ts)
```

A plot of this can help understand the distribution of observations

```{r plot-nobs}
library(ggplot2)
n_key_obs(wages_ts) %>%
  ggplot(aes(x = n_obs)) + 
  geom_bar()
```

# Look at as much of the raw data as possible

## Can we look at a random sample of many people?

With your longitudinal data, you want to look at as much of the raw data as possible.

We know that looking at the `ln_wages` and `xpt` yields a plate of spaghetti:

```{r show-spag}
library(ggplot2)
ggplot(wages_ts, 
       aes(x = xp,
           y = ln_wages,
           group = id)) + 
  geom_line()
```

We might instead want to filter down to those with >10 observations, and then look at a random set of individuals, using the `filter_n_obs()` function.

```{r}
library(tsibble)
library(dplyr)

wages_ts %>% 
  filter_n_obs(n_obs > 10)
```


```{r filter-n-obs}

wages_ts %>%
  filter_n_obs(n_obs > 10) %>%
  ggplot(aes(x = ln_wages,
             y = xp,
             group = id)) + 
  geom_line()
```

But that's still a lot of spaghetti.

We could then sample a set of say 100 observations, using `sample_n_key()`:

```{r sample-n-obs}
wages_ts %>%
  sample_n_keys(size = 100) %>%
  filter_n_obs(n_obs > 10) %>%
  ggplot(aes(x = ln_wages,
             y = xp,
             group = id)) + 
  geom_line()
```

That's not bad, but we can split this up into a few random groups with `stratify_key`, and then plot those groups:

```{r show-add-k-groups}
wages_ts %>%
  filter_n_obs(n_obs > 3) %>%
  sample_n_keys(size = 100) %>%
  stratify_keys(n_strata = 12) %>%
  ggplot(aes(x = ln_wages,
             y = xp,
             group = id)) + 
  geom_line() + 
  facet_wrap(~.strata)
```

# How many missings for each person

# What is the proportion of measurements of (thing) across all observations

Or rather - what is the proportion / tabulation of values across a given measurement? Can we bin these up and explore them?

# What sort of time period is between measurements?

```{r}
wages_ts %>%
  group_by_key() %>%
  summarise(mean_time = mean(xp))
```


# What sort of time period is between each measurement?


# Highlight patterns of interest

We can break the data into groups based on the slope of `xp~ln_wages`

```{r show-add-slope, eval = FALSE}
wages_ts %>%
  filter_n_obs(n_obs > 10) %>%
  add_key_slope(formula = xp~ln_wages) %>%
  mutate(slope_group = if_else(condition = .slope_ln_wages > 0,
                               true = "positive slope",
                               false = "negative slope")) %>%
  ggplot(aes(x = ln_wages,
             y = xp,
             group = id)) + 
  geom_line() + 
  facet_wrap(~slope_group, nrow = 2)
  
```


