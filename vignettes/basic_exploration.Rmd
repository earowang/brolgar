---
title: "Starting Steps"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Starting Steps}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r knitr-set-chunk, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(brolgar)
```

When we first get a longitudinal dataset, you need to understand some of its structure. This vignette demonstrates part of the process of understanding your new longitudinal data.

# Setting up your data

For `brolgar` to work best, you need to 

Before we start, we need to discuss the data structures of brolgar.


# Basic summaries of the data

# How many observations are there?

How many people / ids are there?
How many obserations per person?

the number of observations can mean different things
As we have specified the `key` already - the number of observations, we can use `n_obs()`:

```{r n-obs}
n_obs(ts_wages)
```

Note that this is a single number, in this case, we have `r n_obs(ts_wages)` observations.

```{r}
l_n_obs(ts_wages)
```


If we want the number of observations in each key (id), then we can use `l_n_obs`. This is a special function from `brolgar`, indicated by `l_`. Every `l_` function will return us a dataframe.

A plot of this can help understand the distribution of observations

```{r plot-nobs}
library(ggplot2)
l_n_obs(ts_wages) %>%
  ggplot(aes(x = n_obs)) + 
  geom_bar()
```




# Look at as much of the raw data as possible
# Can we look at a random sample of many people?

With your longitudinal data, you want to look at as much of the raw data as possible.

We know that looking at the `lnw` and `expert` yields a plate of spaghetti:

```{r show-spag}
library(ggplot2)
ggplot(ts_wages, 
       aes(x = exper,
           y = lnw,
           group = id)) + 
  geom_line()
```

We might instead want to filter down to those with >3 observations, and then look at a random set of individuals.

```{r}
library(tsibble)
library(dplyr)


```

We can filter down to those with >10 observations using `filter_n_obs()`:

```{r filter-n-obs}
ts_wages %>%
  filter_n_obs(n_obs > 10) %>%
  ggplot(aes(x = lnw,
             y = exper,
             group = id)) + 
  geom_line()
```

But that's still a lot of spaghetti.

We could then sample a set of say 100 observations, using `sample_n_key()`:

```{r sample-n-obs}
ts_wages %>%
  filter_n_obs(n_obs > 3) %>% 
  sample_n_key(size = 100) %>%
  ggplot(aes(x = lnw,
             y = exper,
             group = id)) + 
  geom_line()
```

That's not bad, but we can split this up into a few random groups with `stratify_key`, and then plot those groups:

```{r show-add-k-groups}
ts_wages %>%
  filter_n_obs(n_obs > 3) %>%
  sample_n_key(size = 100) %>%
  stratify_key(n_strata = 12) %>%
  ggplot(aes(x = lnw,
             y = exper,
             group = id)) + 
  geom_line() + 
  facet_wrap(~.strata)
```

# How many missings for each person

# What is the proportion of measurements of (thing) across all obserations

Or rather - what is the proportion / tabulation of values across a given measurement? Can we bin these up and explore them?

# What sort of time period is between measurements?

# What sort of time period is between each measurement?


# Highlight patterns of interest

We can break the data into groups based on the slope of `exper~lnw`

```{r show-add-slope}
ts_wages %>%
  filter_n_obs(n_obs > 10) %>%
  add_l_slope(id = id, formula = exper~lnw) %>%
  mutate(slope_group = if_else(condition = l_slope_lnw > 0,
                               true = "positive slope",
                               false = "negative slope")) %>%
  ggplot(aes(x = lnw,
             y = exper,
             group = id)) + 
  geom_line() + 
  facet_wrap(~slope_group, nrow = 2)
  
```


